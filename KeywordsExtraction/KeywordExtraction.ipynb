{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93003770-3af0-4f2e-ba78-2d338de6dd2a",
   "metadata": {},
   "source": [
    "# Keyword extraction for Interactive Image Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c79c95-8c82-4f23-ba4e-e8dc62800042",
   "metadata": {},
   "source": [
    "## Setting up and trying things:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dc411-0af1-4ce6-82c1-8becc9972dfc",
   "metadata": {},
   "source": [
    "Let's work with a toy example for an article now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42291a9e-615e-4a4f-a4c2-970a90db2c39",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9a7ac8-8a97-4591-9903-569da49746c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a143a4de-a291-4360-bf1d-d1568ae85513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n         Supervised learning is the machine learning task of \\n         learning a function that maps an input to an output based \\n         on example input-output pairs.[1] It infers a function \\n         from labeled training data consisting of a set of \\n         training examples.[2] In supervised learning, each \\n         example is a pair consisting of an input object \\n         (typically a vector) and a desired output value (also \\n         called the supervisory signal). A supervised learning \\n         algorithm analyzes the training data and produces an \\n         inferred function, which can be used for mapping new \\n         examples. An optimal scenario will allow for the algorithm \\n         to correctly determine the class labels for unseen \\n         instances. This requires the learning algorithm to  \\n         generalize from the training data to unseen situations \\n         in a 'reasonable' way (see inductive bias).\\n      \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186260eb-3040-4421-a490-8f99fa78e667",
   "metadata": {},
   "source": [
    "### Candidate Keywords/Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31580651-b1c1-4a75-b660-7eef9cbb4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9624d62c-6eaa-4828-a0ef-8e44366f0786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['algorithm', 'allow', 'analyzes', 'based', 'bias', 'called',\n",
       "       'class', 'consisting', 'correctly', 'data', 'desired', 'determine',\n",
       "       'example', 'examples', 'function', 'generalize', 'inductive',\n",
       "       'inferred', 'infers', 'input', 'instances', 'labeled', 'labels',\n",
       "       'learning', 'machine', 'mapping', 'maps', 'new', 'object',\n",
       "       'optimal', 'output', 'pair', 'pairs', 'produces', 'reasonable',\n",
       "       'requires', 'scenario', 'set', 'signal', 'situations',\n",
       "       'supervised', 'supervisory', 'task', 'training', 'typically',\n",
       "       'unseen', 'used', 'value', 'vector', 'way'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8cbc1-d105-4c69-b75a-d176c6b2c280",
   "metadata": {},
   "source": [
    "We can use n_gram_range to change the size of the resulting candidates. For example, if we would set it to (3, 3) then the resulting candidates would phrases that include 3 keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c86be-0b9a-4e03-9921-7f0ed21e2225",
   "metadata": {},
   "source": [
    "We can play around with n_gram_range to create different lengths of keyphrases. Then, we might not want to remove stop_words as they can tie longer keyphrases together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fd986-f9e1-4b72-ab6d-0f5ca198ef37",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bee2d-5034-4ce0-98bd-7d914b0cf4b6",
   "metadata": {},
   "source": [
    "Next, we convert both the document as well as the candidate keywords/keyphrases to numerical data. We use BERT for this purpose as it has shown great results for both similarity- and paraphrasing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f59fac-a7c8-4d8e-95ba-b9a728289af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83c88d-e2ad-49d1-bdaa-22691cc81543",
   "metadata": {},
   "source": [
    "We will use Distilbert as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction! \\\n",
    "Since transformer models have a token limit, we might run into some errors when inputting large documents. In that case, we cann consider splitting up the document into paragraphs and mean pooling (taking the average of) the resulting vectors.\n",
    "**NOTE:** There are many pre-trained BERT-based models that we can use for keyword extraction. For our use case the best are either `distilbertâ€”base-nli-stsb-mean-tokens` or `xlm-r-distilroberta-base-paraphase-v1` as they have shown great performance in semantic similarity and paraphrase identification respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74dc021-4d88-4352-88c8-0ac05619c481",
   "metadata": {},
   "source": [
    "### Similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce322dc-064f-4629-94a8-912c84a5825f",
   "metadata": {},
   "source": [
    "As a final step we want to find the candidates that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document.\\\n",
    "To calculate the similarity between candidates and the document, we will be using the cosine similarity between vectors as it performs quite well in high-dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2501f785-f643-458e-a7e9-e2bac39ee3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training', 'algorithm', 'learning']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 3\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13f7da-1431-4686-841e-ba6c6d23b253",
   "metadata": {},
   "source": [
    "The results look great! These terms definitely look like they describe a document about supervised machine learning.\n",
    "Now, let us take a look at what happens if we change the n_gram_range to (3,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62922311-c4c3-498c-9421-bc2c0830841a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_range = (3, 3)\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d6eab-5748-434d-ba61-0d75be44e10f",
   "metadata": {},
   "source": [
    "We notice that the keywords are too similar, we will need to find a way to diversify this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b21a7-4165-48b5-b210-64af77426cd6",
   "metadata": {},
   "source": [
    "### Diversification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83175eb5-d958-49c0-9074-1e6844139c30",
   "metadata": {},
   "source": [
    "We notice that the results are very similar and this is expected as they best represent the document! If we were to diversify the keywords/keyphrases then they are less likely to represent the document well as a collective.\n",
    "Thus, the diversification of our results requires a delicate balance between the accuracy of keywords/keyphrases and the diversity between them.\n",
    "There are two algorithms that we will be using to diversify our results:\n",
    "- Max Sum Similarity\n",
    "- Maximal Marginal Relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff932d5-97d0-4f97-ba62-38411029c982",
   "metadata": {},
   "source": [
    "#### Max Sum Similarity \n",
    "The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad26b233-92ec-48fb-a401-e471ab46ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4d284-c302-4bbe-8091-a097b1d6afae",
   "metadata": {},
   "source": [
    "To do this, we select some top keywords/keyphrases, and from those, select the 5 that are the least similar to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce25901b-eda2-4e39-b9f3-6dc991410274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requires learning algorithm',\n",
       " 'signal supervised learning',\n",
       " 'learning function maps',\n",
       " 'algorithm analyzes training',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee5b6e-a1bf-4c41-bf4d-43a6412f8274",
   "metadata": {},
   "source": [
    "-> If we set a **low** `nr_candidates`, then our results seem to be very similar to our original cosine similarity method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d962efa7-04cf-482e-b569-1b6ab1cb2ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set training examples',\n",
       " 'generalize training data',\n",
       " 'requires learning algorithm',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164abb18-6946-4e31-a795-38a09a951716",
   "metadata": {},
   "source": [
    "-> However, a relatively **high** `nr_candidates` will create more diverse keyphrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fb6c0-b000-4aa4-b336-95920474b05a",
   "metadata": {},
   "source": [
    "#### Maximal Marginal Relevance\n",
    "We can diversify our results using Maximal Marginal Relevance (MMR). MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. \\\n",
    "We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "211acb88-098d-4311-be52-59e0e348275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f863dd-4938-4fc7-9205-da83fbfe6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'learning algorithm generalize']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d856f08-8848-41dd-8b4b-6a85e4c55a54",
   "metadata": {},
   "source": [
    "If we set a relatively **low diversity**, then our results seem to be very similar to our original cosine similarity method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f895d2-aa1b-4817-854f-7c5878c59f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'labels unseen instances',\n",
       " 'new examples optimal',\n",
       " 'determine class labels',\n",
       " 'supervised learning algorithm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321fc1d-d5ab-4380-8a30-99086e507d92",
   "metadata": {},
   "source": [
    "However, a relatively **high diversity** score will create very diverse keyphrases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
